# **Behavioral Cloning** 

The primary goal of the project is to use a simulator to collect data to train and validate a network that clones the good driving behavior of the driver. The data collected from the simulator includes centre, left, right images captured by the camera, Steering Angle, Throttle, Brake, Speed of the car. The model built uses images collected from the centre, left, right camera along with steering angle for training and validation.  

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report

## Files Submitted & Code Quality

### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* Images folder containing sample images used in training.
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_report.md summarizing the results
* run1.mp4 is the video recording of the vehicle driving autonomously 

### 2. Submission includes functional code

Using the Udacity provided simulator and the drive.py file, the car can be driven autonomously around the track by executing 
* python drive.py model.h5 run1

A video recording of the car  autonomously is generated by executing 
* python video.py run1

### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

## Model Architecture and Training Strategy

### 1. An appropriate model architecture has been employed

A deep convolution neural network model was built with following layers:
 
* First layer of the model includes Keras lambda layer for normalization to avoid saturation and make gradients work better.
* The second layer is Keras covolutional Cropping2D layer which crops the image along spatial dimensions.
* The model further consists of a convolution neural network with 5x5 filter sizes, depths between 6 and 16 with a RELU activation layer to introduce non-linearity 
* The convolutional layer is followed by Max pooling with a pool size of (2,2) with padding = 'valid' 
* The model includes dropout layer with a dropout of 50% to avoid overfitting.
*  The model has four fully connected layers with neurons (120,84,10,1) after flattening the output of convolutional layers.

#### 2. Attempts to reduce overfitting in the model

The model contains dropout layers in order to reduce overfitting.

The model was trained and validated on different data sets with different batch size and epochs to ensure that the model was not overfitting. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually.

#### 4. Appropriate training data

Training data was chosen to keep the vehicle driving on the road. I used a combination of clockwise center lane driving, recovering from the left and right sides of the road and anti-clockwise center lane driving,

The driving data includes images from centre, left, right camera with appropriate steering angle and recovering from the sides of the road helps the model to steer if the car drifts off to the left or the right. 

Track one is primarily driving clock-wise which is used to get the training and validation data. Hence the data collected is left turn bias. One way to combat the bias is to turn the card around and record counter-clockwise laps around the track. This gives the model a new track to learn from, such that the model gets the opportunity to generalized better. 

The data is collected via:

* two laps of center lane driving clockwise
* one lap focusing on center lane driving counter-clockwise
* one lap of recovery driving from the sides


### Model Architecture and Training Strategy

#### 1. Solution Design Approach

My first step was to use a convolution neural network model similar to the LeNet Model. The initial model was trained with images from centre camera.

In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. 

I found that my first model had a low mean squared error on the training set but a high mean squared error on the validation set. This implied that the model was overfitting. To combat the overfitting, I added a dropout layer to the model. 

The final step was to run the simulator to see how well the car was driving around track one. There were a few spots where the vehicle fell off the track. To improve the driving behavior in these cases, I combined the left and right camera images to the data set with appropriate steering angle. 

At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

#### 2. Creation of the Training Set & Training Process

To capture good driving behavior, I first recorded two laps on track one using clockwise center lane driving. Here is an example image of center lane driving:

![](images/center_clockwise.jpg?raw=true "center_clockwise")

I then recorded the vehicle recovering from the left side and right sides of the road back to center so that the vehicle would learn to steer if the car drifts off to the left or the right. Also, I used the left and right camera images to the data set with appropriate steering angle.  

![](images/left_clockwise.jpg?raw=true "left_clockwise")
![](images/right_clockwise.jpg?raw=true "right_clockwise")

Then I repeated the process of center lane driving counter-clockwise on track one in order to generalize the model.

![](images/center_counter_clockwise.jpg?raw=true "center_counter_clockwise")
![](images/left_counter_clockwise.jpg?raw=true "left_counter_clockwise")
![](images/right_counter_clockwise.jpg?raw=true "right_counter_clockwise")

After the collection process, I had 12281 number of data points. I then preprocessed this data by normalization. In the data images, the top portion of the image captures trees and hills and sky, and the bottom portion of the image captures the hood of the car. In order to train faster each image is cropped to focus on only the portion of the image that is useful for predicting a steering angle.

I finally randomly shuffled the data set and used train_test_split function to randomly split the data into validation set(3685 samples) and training set (33158 samples)

I used this training data for training the model. The validation set helped determine if the model was over or under fitting. I used an adam optimizer so that manually training the learning rate wasn't necessary.


#### 3. Final Model Architecture 
 
Train on 33158 samples, validate on 3685 samples

Epoch 1/10
709s - loss: 6.3906 - val_loss: 0.0461
Epoch 2/10
606s - loss: 0.0405 - val_loss: 0.0373
Epoch 3/10
538s - loss: 0.0352 - val_loss: 0.0342
Epoch 4/10
506s - loss: 0.0326 - val_loss: 0.0324
Epoch 5/10
494s - loss: 0.0310 - val_loss: 0.0313
Epoch 6/10
514s - loss: 0.0298 - val_loss: 0.0301
Epoch 7/10
495s - loss: 0.0285 - val_loss: 0.0291
Epoch 8/10
512s - loss: 0.0276 - val_loss: 0.0284
Epoch 9/10
493s - loss: 0.0268 - val_loss: 0.0278
Epoch 10/10
484s - loss: 0.0261 - val_loss: 0.0273
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
lambda_1 (Lambda)            (None, 160, 320, 3)       0
_________________________________________________________________
cropping2d_1 (Cropping2D)    (None, 65, 320, 3)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 61, 316, 6)        456
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 30, 158, 6)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 26, 154, 16)       2416
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 13, 77, 16)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 13, 77, 16)        0
_________________________________________________________________
flatten_1 (Flatten)          (None, 16016)             0
_________________________________________________________________
dense_1 (Dense)              (None, 120)               1922040
_________________________________________________________________
dense_2 (Dense)              (None, 84)                10164
_________________________________________________________________
dense_3 (Dense)              (None, 10)                850
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 11
=================================================================
Total params: 1,935,937.0
Trainable params: 1,935,937.0
Non-trainable params: 0.0
_________________________________________________________________